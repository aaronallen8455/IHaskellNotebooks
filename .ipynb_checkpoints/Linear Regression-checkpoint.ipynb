{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "###### (https://samcgardner.github.io/2018/10/06/linear-regression-in-haskell.html)\n",
    "\n",
    "The point of linear regression is the model the relationship between continuous values by assuming that one is a linear function of the other. We want to find a line that has the best fit for our data.\n",
    "\n",
    "The equation for such a line is representable as `y = mx + b` so we need a way to determine the best matches for what `m` and `b` should be. For this we will use Gradient Descent.\n",
    "\n",
    "Gradient descent can be applied to all differentiable functions. It says that a minima exists where all the partial derivatives are equal to zero and that by going in the direction of negative gradients we will eventually reach a minima if one exists.\n",
    "\n",
    "What we need is a function whose value is at a minimum when the values for `m` and `b` are optimal. This is called the 'Cost Function.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coefficients (49.99999,10.000008)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ":m Data.List\n",
    "\n",
    "-- represents our m and b coefficients\n",
    "newtype Coefficients = Coefficients (Float, Float) deriving Show\n",
    "\n",
    "-- a data point in our training set\n",
    "newtype Example = Example (Float, Float)\n",
    "\n",
    "newtype TrainingSet = TrainingSet [Example]\n",
    "\n",
    "-- Finds the coefficients for linear regression using gradient descent\n",
    "linearRegression :: Coefficients -> Float -> TrainingSet -> Int -> Coefficients\n",
    "linearRegression coefficients alpha dataset iterations\n",
    "  | iterations == 0 = coefficients\n",
    "  | otherwise =\n",
    "    let thetas = newThetas coefficients alpha dataset\n",
    "    in linearRegression thetas alpha dataset (iterations - 1)\n",
    "    \n",
    "-- calculate new values for t0 and t1\n",
    "newThetas :: Coefficients -> Float -> TrainingSet -> Coefficients\n",
    "newThetas thetas@(Coefficients (t0, t1)) alpha (TrainingSet examples) =\n",
    "  let deltas = map (calculateDelta thetas) examples\n",
    "      adjustedDeltas = adjustDeltas deltas examples\n",
    "      newt0 = t0 - alpha * avg deltas\n",
    "      newt1 = t1 - alpha * avg adjustedDeltas\n",
    "  in Coefficients (newt0, newt1)\n",
    "\n",
    "-- Calculates the difference between h(x) and y\n",
    "calculateDelta :: Coefficients -> Example -> Float\n",
    "calculateDelta (Coefficients (t0, t1)) (Example (x, y)) = t0 + t1 * x - y\n",
    "\n",
    "-- For the case where d/dx != 1 and we need to mulitply through, do so\n",
    "adjustDeltas :: [Float] -> [Example] -> [Float]\n",
    "adjustDeltas deltas examples =\n",
    "  let xs = map (\\(Example (x, _)) -> x) examples\n",
    "      zipped = zip deltas xs\n",
    "  in map (uncurry (*)) zipped\n",
    "  \n",
    "avg xs = realToFrac (sum xs) / genericLength xs\n",
    "\n",
    "-- now test it\n",
    "let thetas = Coefficients (0, 0)\n",
    "let alpha = 0.3\n",
    "let trainingset =\n",
    "      TrainingSet [Example (0, 50), Example (1, 60), Example (2, 70)]\n",
    "let iterations = 500\n",
    "\n",
    "linearRegression thetas alpha trainingset iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
